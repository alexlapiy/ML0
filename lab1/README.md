# Метрические алгоритмы классификации
  Метрический алгоритм - алгоритм классификации основанный на оценке близости объектов. Для формализации понятия «оценки близости» вводится функция расстояния в пространстве объектов *X*. Простейшим метрическим классификатором является метод ближайших соседей, в котором классифицируемый объект относится к тому классу, которому принадлежит большинство схожих с ним объектов. Метрические классификаторы опираются на гипотезу компактности, которая предполагает, что схожие объекты чаще лежат в одном классе, чем в разных. Это означает, что граница между классами имеет достаточно простую форму, и классы образуют компактно локализованные области в пространстве объектов. 
## Метод ближайших соседей (kNN - k Nearest Neighbours)

- **kNN** - метрический алгоритм классификации, основанный на оценивании сходства объектов. Классифицируемый объект относится к тому классу, которому принадлежат ближайшие к нему *k* объектов из обучающей выборки. KNN также как и kwNN является ленивым алгоритмом, то есть обучение начинается только тогда, когда приходит объект *u* для классификации. Алгоритм зависит от параметра k, оптимальное значение которого определяется по критерию скользящего контроля, в данном случае используется метод исключения объектов по одному (LOO).

В общем виде функция нахождения **_kNN_** выглядит так:
<img src="https://github.com/alexlapiy/ML0/blob/master/screens/lab1/knn_formula.png" width="300" height="70"> с весовой функцией <img src="https://github.com/alexlapiy/ML0/blob/master/screens/lab1/knn_weight.png" width="150" height="35">

**Алгоритм**
- Получаем объект для классификации, считаем евклидово расстояние от классифицируемого объекта до каждого элемента из выборки
- Сортируем по расстоянию
- Берем первые *k* объектов
- Считаем встречаемость каждого класса
- Берем самый встречаемый, данный класс и будет являться ответом.

### График для LOO kNN
![](https://github.com/alexlapiy/ML0/blob/master/screens/lab1/LOO(K).png "LOO(K)")

### Карта классификации для KNN

![](https://github.com/alexlapiy/ML0/blob/master/screens/lab1/classificatinMapKnn.png "classificationMapKnn")

## Метод *k* взвешенных ближайших соседей (kwNN - k weighted Nearest Neighbours);
- **kwNN** - также является метрическим алгоритмом классификации, однако в отличии от kNN, оценивает степень важность каждого объекта обучающей выборки, используя параметр *w* - вес объекта. В общем виде функция нахождения **_kwNN_** выглядит так:
<img src="https://github.com/alexlapiy/ML0/blob/master/screens/lab1/kwnn_formula.png" width="300" height="70"> с весовой функцией <img src="https://github.com/alexlapiy/ML0/blob/master/screens/lab1/kwnn_weight.png" width="150" height="35">

q - знаменатель прогрессии, некоторое число ( 0 < q < 1). Оптимальное q также подбирается при помощи LOO. 

**Алгоритм**
- Получаем объект для классификации, считаем евклидово расстояние от классифицируемого объекта до каждого элемента из выборки
- Сортируем по расстоянию
- Берем первые *k* объектов
- Составляем пустую таблицу встречаемости каждого класса
- Заполняем ее добавляя к объекту вес (убывающую прогрессию)
- Возвращаем объект с максимальным весом, данный класс и будет являться ответом.

### График для LOO kwNN

![](https://github.com/alexlapiy/ML0/blob/master/screens/lab1/LOO(Q).png "LOO(q)")

Таким образом *q* = 0.6

### Карта классификации для kwNN

![](https://github.com/alexlapiy/ML0/blob/master/screens/lab1/classificatinMapKwnn.png "classificationMapKwnn")

## Пример преимущества kwNN над kNN
На графике видно, что при K > 4, kNN алгоритм дает не верный ответ, поскольку объектов другого класса больше, чем объектов класса рядом с искомым значением. Алгоритм kwNN в данном случае справляется лучше, поскольку, например при оптимальном q = 0.6, первый объект из первого класса будет иметь вес 0.6, второй объект из первого класса - 0.36, в сумме - 0.96, что больше 0.47 для другого класса.
<img src="https://github.com/alexlapiy/ML0/blob/master/screens/lab1/knn_vs_kwnn.jpg" width="1000" height="400">
